# Setting
SAMPLE = False

# Import libraries
from setting import *
from util import *

import gc
import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K

os.environ['PYTHONHASHSEED']=str(SEED)
tf.set_random_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Activation
from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, Callback
import lightgbm as lgb


# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.options.display.max_rows = 100
pd.options.display.max_columns = 100
pd.options.display.width = 1000


# Set Global Random Seed -------------------------------------------------------
print_title('Set Global Random Seed')
np.random.seed(SEED)
print_info('Set global random seed to {}'.format(SEED))

# Load data --------------------------------------------------------------------
print_title('Load Data')
print_info('Start loading data')

if not os.path.isfile(TRAIN_DATA_PATH):
    print_info("File '{}' does not exist.".format(TRAIN_DATA_PATH))
    print_info("Please download it according to the instructions in README.md")
    quit()

if SAMPLE:
    train_df = pd.read_csv(TRAIN_DATA_PATH, nrows = 100000, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])
else:
    train_df = pd.read_csv(TRAIN_DATA_PATH, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])

print_info('Finish loading data')

# Get data summary of missing frequency and unique levels ----------------------
if not os.path.isfile(os.path.join(CHECK_DATA_DIRECTORY, 'data_summary.csv')):
    print_info('Creating data summary')
    summary_df = train_df.describe(include = 'all')
    summary_df = summary_df.transpose()
    summary_df.index.name = 'Column'
    summary_df['count'] = summary_df['count'].astype(int)
    missing_percentage_df = get_null_percentage(train_df)
    missing_percentage_df = missing_percentage_df.set_index('Column')
    summary_df = pd.concat([summary_df, missing_percentage_df], axis = 1).reset_index()

# Preprocess data
print_info('Start preprocessing data')
from preprocess import *
select_features(train_df)
fill_missing_values(train_df)
aggregate(train_df)
map_values(train_df)
handle_invalid_values(train_df)
remove_outliers(train_df)
encode_categorical_values(train_df)
print_info('Finish preprocessing data')

# Split Train and Test ---------------------------------------------------------
print_title('Split Train and Test')
print_info('Start splitting data')
y = train_df[TARGET]
x = train_df.drop(columns = TARGET)
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3, stratify = y, random_state = SEED)
print_info('Trainser shape: {} | Testset shape: {}'.format(train_x.shape, test_x.shape))
print_info('Trainset Distribution: ')
print(train_y.value_counts() / train_y.shape[0])
print_info('Testset Distribution: ')
print(test_y.value_counts() / test_y.shape[0])

print(train_x['AvSigVersion_time_series'].iloc[:3])
normalize_data(train_x, test_x)

del(train_df, y, x)
gc.collect()

# # PCA --------------------------------------------------------------------------
# print_info('Finding n_components that could explain at least 90% variances')
# pca = PCA(n_components=train_x.shape[-1])
# pca.fit(train_x)
# ratio_cumsum = pca.explained_variance_ratio_.cumsum()
# ratio_df = pd.DataFrame(data = { 'No. of Components': range(1, len(pca.explained_variance_ratio_)+1),'Explained Variance Ratio': pca.explained_variance_ratio_, 'Cum Explained Variance Ratio': ratio_cumsum})
# ratio_df.to_csv(os.path.join(RESULTS_DIRECTORY, 'PCA_Explained_Variance_Ratio.csv'), index = False)
# plot_pca_ratio_cumsum(ratio_cumsum, save_directory=RESULTS_IMAGE_DIRECTORY, show = False)
#
# N_COMPONENTS = sum(ratio_cumsum<0.95)+1
# pca = PCA(n_components=N_COMPONENTS)
# pca.fit(train_x)
# ratio_cumsum = pca.explained_variance_ratio_.cumsum()
# print_info('Explained variance by {} n_components: {:.3f}'.format(N_COMPONENTS, ratio_cumsum[-1]))

# Build Model ------------------------------------------------------------------
# def pca_transform(train_x, test_x):
#     pca = PCA(n_components=N_COMPONENTS)
#     train_x = pca.fit_transform(train_x)
#     test_x = pca.transform(test_x)
#     print_info('PCA transformed train_x shape: {}'.format(train_x.shape))
#     return train_x, test_x

def grid_search(estimator_name, estimator, train_x, train_y, pca):
    print_info('Start Grid Searching {}'.format(estimator_name))
    gridsearcher = GridSearchCV(estimator, param_grid = estimators_params_grid[estimator_name], **gridsearch_param)
    gridsearcher.fit(train_x, train_y)
    cv_results_df = pd.DataFrame(gridsearcher.cv_results_)

    cv_results_fpath = os.path.join(RESULTS_FT_DIRECTORY, 'cv_results-{}{}.csv'.format(estimator_name, '+PCA' if pca else ''))
    try:
        current_cv_results_df = pd.read_csv(cv_results_fpath)
        cv_results_df = pd.concat([current_cv_results_df, cv_results_df])
        print_info('Appending to the existing cv_results.')
    except:
        print_info('There is no existing cv_results to append to. Creating a new one.')
    cv_results_df.to_csv(cv_results_fpath, index = False)

    save_obj(gridsearcher.best_params_, os.path.join(RESULTS_FT_DIRECTORY, 'FT-Best_Params-{}{}.pkl'.format(estimator_name, '+PCA' if pca else '')))
    save_obj(gridsearcher.best_estimator_.get_params(), os.path.join(RESULTS_FT_DIRECTORY, 'Params-{}{}.pkl'.format(estimator_name, '+PCA' if pca else '')))

    print_info('Grid Search best AUC Score: {}'.format(gridsearcher.best_score_))
    print('Best Params: \n{}'.format(gridsearcher.best_params_))

    return gridsearcher.best_estimator_, gridsearcher.best_estimator_.get_params()

def make_prediction(model, train_x, test_x, estimator_name, pca=False, fine_tune=False):
    print_info('Start Making Prediction with Best Estimator {}'.format(estimator_name))

    if estimator_name == 'NeuralNetwork':
        train_predict = model.predict(train_x)
        test_predict = model.predict(test_x)
    else:
        train_predict = model.predict_proba(train_x)[:, -1]
        test_predict = model.predict_proba(test_x)[:, -1]

    return train_predict, test_predict

def save_score(estimator_name, pca, fine_tune, train_roc_score, test_roc_score):
    scores_fpath = os.path.join(RESULTS_DIRECTORY, 'scores.csv')
    try:
        scores_df = pd.read_csv(scores_fpath)
    except:
        scores_df = pd.DataFrame(columns = ['Model', 'PCA', 'Fine Tune', 'Train AUC Score', 'Test AUC Score'])

    scores_df.loc[scores_df.shape[0]] = [estimator_name, pca, fine_tune, train_roc_score, test_roc_score]
    scores_df.to_csv(scores_fpath, index = False)

def process_prediction(model, train_prob, train_y, test_prob, test_y, estimator_name, pca=False, fine_tune=False):

    plot_roc_curve(train_y, train_prob, save_directory = RESULTS_IMAGE_DIRECTORY, show = False, title = '{} ROC Curve - {}{}{}'.format('Train', estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else ''))
    plot_roc_curve(test_y, test_prob, save_directory = RESULTS_IMAGE_DIRECTORY, show = False, title = '{} ROC Curve - {}{}{}'.format('Test', estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else ''))

    train_roc_score = roc_auc_score(train_y, train_prob)
    test_roc_score = roc_auc_score(test_y, test_prob)

    # Save settings and score
    save_score(estimator_name, pca, fine_tune, train_roc_score, test_roc_score)

    print_info('AUC Score for {}: {:.3f} (Train)'.format(estimator_name, train_roc_score))
    print_info('AUC Score for {}: {:.3f} (Test)'.format(estimator_name, test_roc_score))

    # Save prediction results
    np.save(os.path.join(RESULTS_PREDICTION_DIRECTORY, 'train-{}{}{}'.format(estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else '')), train_prob)
    np.save(os.path.join(RESULTS_PREDICTION_DIRECTORY, 'test-{}{}{}'.format(estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else '')), test_prob)

def process_feature_importances(model, estimator_name, pca, fine_tune):
    if estimator_name == 'LogisticRegression':
        importances = model.coef_
    else:
        importances = model.feature_importances_
    importances_df = pd.DataFrame(data = {'Column': list(train_x.columns), 'Importance': importances.reshape(-1,)})
    importances_df.sort_values('Importance', ascending = False, inplace = True)
    importances_df.to_csv(os.path.join(RESULTS_IMPORTANCE_DIRECTORY, '{}{}{}.csv'.format(estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else '')), index = False)

from tensorflow import numpy_function, double, unique, rint
def auroc(y_true, y_pred):
    if len(unique(y_true[0])) !=2 :
        return numpy_function(accuracy_score, (y_true, rint(y_pred)), double)
    return numpy_function(roc_auc_score, (y_true, y_pred), double)


def build_nn(train_x):
    model = Sequential()

    model.add(Dense(128,input_dim=train_x.shape[1]))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(128))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auroc])

    return model

def plot_nn_history(history):
    import matplotlib.pyplot as plt

    auroc = history.history['auroc']
    val_auroc = history.history['val_auroc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1,len(auroc)+1)

    fig = plt.figure(facecolor='w', figsize=(PLOT_WIDTH, PLOT_HEIGHT))
    plt.plot(epochs, auroc,'r',label='Training AUC')
    plt.plot(epochs, val_auroc,'b',label='Validation AUC')
    plt.ylabel('No. of Epochs', fontsize=16)
    plt.xlabel('AUC Score', fontsize=16)
    plt.tick_params(labelsize=16)
    title = 'AUC Score by No. of Epochs'
    plt.title(title, loc = 'center', y=1.1, fontsize = 20)
    plt.tight_layout()
    plt.legend()
    saved_path = os.path.join(RESULTS_IMAGE_DIRECTORY, convert_filename(title))
    fig.savefig(saved_path, dpi=200, bbox_inches="tight")
    print('Saved to {}'.format(saved_path))

    fig = plt.figure(facecolor='w', figsize=(PLOT_WIDTH, PLOT_HEIGHT))
    plt.plot(epochs, loss,'r',label='Training Loss')
    plt.plot(epochs, val_loss,'b',label='Validation Loss')
    plt.ylabel('No. of Epochs', fontsize=16)
    plt.xlabel('Binary Cross Entropy', fontsize=16)
    plt.tick_params(labelsize=16)
    title = 'Binary Cross Entropy by No. of Epochs'
    plt.title(title, loc = 'center', y=1.1, fontsize = 20)
    plt.tight_layout()
    plt.legend()
    saved_path = os.path.join(RESULTS_IMAGE_DIRECTORY, convert_filename(title))
    fig.savefig(saved_path, dpi=200, bbox_inches="tight")
    print('Saved to {}'.format(saved_path))

    plt.show()

def train_nn(model, train_x, train_y):
    checkpoint_path = os.path.join(RESULTS_DIRECTORY, "nn/weights.ckpt")
    checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=0, save_best_only=True)
    earlystopping = EarlyStopping(patience = 5, verbose=2, monitor='val_auroc', mode='max', restore_best_weights = True)
    tensorboard = TensorBoard(log_dir=os.path.join(RESULTS_DIRECTORY, 'nn/logs'))

    history = model.fit(train_x, train_y, epochs = EPOCHS, batch_size=128, validation_split = 0.3,
                        callbacks = [checkpoint, earlystopping, tensorboard])
    plot_nn_history(history)
    return model

def find_lgb_best_n_estimators(estimator, train_x, train_y, test_x):
    FOLDS = 3
    folds = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)

    count = 0
    n_estimators = 0
    for train_index, val_index in folds.split(train_x, train_y):
        # TRAIN LGBM
        count += 1
        print_info('LGBMClassifier FOLD {}'.format(count))
        train_x_A, train_x_B = train_x.iloc[train_index], train_x.iloc[val_index]
        train_y_A, train_y_B = train_y.iloc[train_index], train_y.iloc[val_index]

        estimator.fit(train_x_A, train_y_A, eval_metric='auc', eval_set=[(train_x_B, train_y_B)], verbose=1, early_stopping_rounds=100)
        print_info('Best n_estimators = {}'.format(estimator.best_iteration_))
        n_estimators += estimator.best_iteration_/FOLDS

    print_info('Averaged best n_estimators = {}'.format(round(n_estimators)))

    return round(n_estimators)

def experiment(estimator, train_x, train_y, test_x, test_y, pca = False, fine_tune = False):
    if estimator == 'NeuralNetwork':
        estimator_name = 'NeuralNetwork'
    else:
        estimator_name = estimator.__class__.__name__

    print_sub_title('Experimenting {}; PCA Mode: {}; Fine Tune Mode: {}'.format(estimator_name, pca, fine_tune))

    # Transform if PCA
    if pca:
        train_x, test_x = pca_transform(train_x, test_x)

    if estimator_name == 'NeuralNetwork':
        model = train_nn(build_nn(train_x), train_x, train_y)
    elif estimator_name == 'LGBMClassifier':
        if fine_tune:
            model, best_params = grid_search(estimator_name, estimator, train_x, train_y, pca)
            estimator = estimator.__class__(**best_params)
            print(estimator.get_params())
            # return

        # print_info('Start Finding best n_estimators {}'.format(estimator_name))
        # estimator.n_estimators=10000
        # print(estimator.get_params())
        # best_n_estimators = find_lgb_best_n_estimators(estimator, train_x, train_y, test_x)
        # estimator.n_estimators = best_n_estimators
        # save_obj(estimator.get_params(), os.path.join(RESULTS_FT_DIRECTORY, 'Params-{}{}.pkl'.format(estimator_name, '+PCA' if pca else '')))
        # print(estimator.get_params())

        print_info('Start Fitting {}'.format(estimator_name))
        model = estimator.fit(train_x, train_y)
    elif not fine_tune:
        print_info('Start Fitting {}'.format(estimator_name))
        model = estimator.fit(train_x, train_y)
    else:
        model, best_params = grid_search(estimator_name, estimator, train_x, train_y, pca)


    train_prob, test_prob = make_prediction(model, train_x, test_x, estimator_name, pca, fine_tune)
    process_prediction(model, train_prob, train_y, test_prob, test_y,  estimator_name, pca, fine_tune)

    if not pca and estimator_name not in ['GaussianNB', 'NeuralNetwork']:
        process_feature_importances(model, estimator_name, pca, fine_tune)

gridsearch_param = {'scoring': 'roc_auc', 'verbose': 2 , 'n_jobs': -1, 'cv': 3}
estimators_params_grid = {
    'LogisticRegression': {'C' : [10**i for i in range(3,4)], 'solver': ['saga'], 'penalty': ['l2', 'l1'], 'max_iter': [500]},
    'DecisionTreeClassifier': {'min_samples_split': [1600, 1800, 2000, 2200, 2400]},
    'RandomForestClassifier': {'n_estimators' : [50,100,200,300], 'min_samples_split': [100, 200, 300]},
    'LGBMClassifier': {'num_leaves': [2500], 'n_estimators': [600, 800, 1000, 200]},
    }

print_info('Start experiments')

# experiment(LogisticRegression(random_state=SEED, n_jobs=-1), train_x, train_y, test_x, test_y, pca = False, fine_tune = True)
# experiment(DecisionTreeClassifier(random_state=SEED), train_x, train_y, test_x, test_y, pca = False, fine_tune = True)
# experiment(GaussianNB(), train_x, train_y, test_x, test_y, pca = False, fine_tune = False)
# experiment(RandomForestClassifier(random_state=SEED, n_jobs=-1), train_x, train_y, test_x, test_y, pca = False, fine_tune = True)
#
# Fine tune to find best n_estimators
# lgbm = lgb.LGBMClassifier(objective='binary',
#                           random_state = SEED,
#                           feature_fraction=0.7,
#                           learning_rate=0.05,
#                           n_jobs=-1,
#                           silent = False,
#                           )
# experiment(lgbm, train_x, train_y, test_x, test_y, pca = False, fine_tune = True)
# 
# """ Bagging with Lightgbm (Combine boosting and bagging)"""
# print_info('Start Bagging with Lightgbm')
# lgbm = lgb.LGBMClassifier(**load_obj(os.path.join(RESULTS_FT_DIRECTORY, 'Params-{}{}.pkl'.format('LGBMClassifier', ''))))
# # print(lgbm)
# model = BaggingClassifier(lgbm, random_state=SEED, n_jobs=-1, n_estimators=10)
# model.fit(train_x, train_y)
# estimator_name = 'LGBM (Bagging)'
# train_prob, test_prob = make_prediction(model, train_x, test_x, estimator_name)
# process_prediction(model, train_prob, train_y, test_prob, test_y,  estimator_name)

def build_nn(train_x):
    model = Sequential()

    model.add(Dense(64,input_dim=train_x.shape[1]))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(32))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(16))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auroc])

    return model

EPOCHS = 1000
experiment('NeuralNetwork', train_x, train_y, test_x, test_y, pca = False, fine_tune = False)



# --------------- Experimenting NeuralNetwork; PCA Mode: False; Fine Tune Mode: False ----------------
# Train on 4371526 samples, validate on 1873512 samples
# Epoch 1/1000
# 4371526/4371526 [==============================] - 433s 99us/step - loss: 0.6483 - accuracy: 0.6141 - auroc: 0.6663 - val_loss: 0.6434 - val_accuracy: 0.6206 - val_auroc: 0.6769
# Epoch 2/1000
# 4371526/4371526 [==============================] - 337s 77us/step - loss: 0.6427 - accuracy: 0.6218 - auroc: 0.6765 - val_loss: 0.6415 - val_accuracy: 0.6222 - val_auroc: 0.6815
# Epoch 3/1000
#  892736/4371526 [=====>........................] - ETA: 5:06 - loss: 0.6409 - accuracy: 0.6232 - auroc: 0.6790W1026 11:35:42.244776 4533624128 deprecation_wrapper.py:119] From /Users/clarencecastillo/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.
#

#
# """Obtain Highest Score Setting for Each Model"""
# scores_df = pd.read_csv(os.path.join(RESULTS_DIRECTORY, 'scores.csv'))
# highest_scores_df = scores_df.groupby(['Model'])['Test AUC Score'].max().reset_index()
# highest_scores_df = highest_scores_df.merge(scores_df)
# highest_scores_df.sort_values('Test AUC Score', inplace = True, ascending = False)
# highest_scores_df.to_csv(os.path.join(RESULTS_DIRECTORY, 'scores_highest_model_setting.csv'), index = False)
#
# """Average prediction"""
# def load_prediction(estimator_name, is_test, pca, fine_tune):
#     return np.load(os.path.join(RESULTS_PREDICTION_DIRECTORY, '{}-{}{}{}.npy'.format('test' if is_test else 'train', estimator_name, '+PCA' if pca else '', '+FT' if fine_tune else '')))
#
# test_predictions = {}
# for index, row in highest_scores_df.iterrows():
#     test_predictions[row['Model']] = load_prediction(row['Model'], True, row['PCA'], row['Fine Tune']).reshape(-1,)
# test_predictions_df = pd.DataFrame(data = test_predictions)
#
# train_predictions = {}
# for index, row in highest_scores_df.iterrows():
#     train_predictions[row['Model']] = load_prediction(row['Model'], False, row['PCA'], row['Fine Tune']).reshape(-1,)
# train_predictions_df = pd.DataFrame(data = train_predictions)
#
# scores = []
# for i in range(len(test_predictions)):
#     estimators = highest_scores_df.iloc[:i+1]['Model'].unique()
#     test_averaged_prob = test_predictions_df[estimators].mean(axis = 1)
#     scores.append(roc_auc_score(test_y, test_averaged_prob))
# scores_df = pd.DataFrame(data = {'No. of Models': list(range(1, len(scores)+1)), 'Score': scores})
# scores_df.sort_values('Score', inplace = True, ascending = False)
# scores_df.to_csv(os.path.join(RESULTS_DIRECTORY, 'scores_by_average.csv'), index = False)
#
# estimator_name = 'Average'
# best_n_models = int(scores_df.iloc[0]['No. of Models'])
# test_prob = test_predictions_df[highest_scores_df.iloc[:best_n_models]['Model']].mean(axis = 1)
# train_prob = train_predictions_df[highest_scores_df.iloc[:best_n_models]['Model']].mean(axis = 1)
# process_prediction(model, train_prob, train_y, test_prob, test_y,  estimator_name)
#
# """Stacking"""
# lr = LinearRegression()
# lr.fit(train_predictions_df, train_y)
# lr.fit(train_predictions_df[['LGBM (Bagging)', 'NeuralNetwork']], train_y)
# lr.coef_
# roc_auc_score(test_y, lr.predict(test_predictions_df[['LGBM (Bagging)', 'NeuralNetwork']]))
#
#
# """Obtain Highest Score Setting for Each Model"""
# scores_df = pd.read_csv(os.path.join(RESULTS_DIRECTORY, 'scores.csv'))
# highest_scores_df = scores_df.groupby(['Model'])['Test AUC Score'].max().reset_index()
# highest_scores_df = highest_scores_df.merge(scores_df)
# highest_scores_df.sort_values('Test AUC Score', inplace = True, ascending = False)
# highest_scores_df.to_csv(os.path.join(RESULTS_DIRECTORY, 'scores_highest_model_setting.csv'), index = False)
