""" Setting """
SAMPLE = True
SEED = 2019

""" import libraries """
import random
import numpy as np
import pandas as pd
from datetime import date, timedelta, datetime
from tqdm import tqdm
from itertools import product
import gc
import re
import random
import os
import pickle
from util import *
import time

from hypopt import GridSearch
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.tree import DecisionTreeRegressor
import tensorflow as tf
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.python.keras.layers import BatchNormalization
from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor
import lightgbm as lgb

pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 50)

import warnings
warnings.filterwarnings('ignore')

"""Prepare & Clean data
- Load Data
- Create observations for unseen items
- Fill na with 0 for unit_sales of unseen items, and False for missing onpromotion
"""
print('{}: Start reading data'.format(current_time()))
if not SAMPLE:
    train_all = pd.read_csv(TRAIN_DATA_PATH,
                           usecols=[1, 2, 3, 4, 5],
                           dtype=LOAD_TYPES,
                           converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},
                           parse_dates=["date"],
                           # skiprows=range(1, 66458909)  # 2016-01-01 (59038132, 5)
                           skiprows=range(1, 101688780)  # 2017-01-01 (59038132, 5)
                           )
else:
    train_all = pd.read_csv(TRAIN_DATA_PATH_SAMPLE_item30, dtype=LOAD_TYPES, parse_dates=["date"])
test = pd.read_csv(TEST_DATA_PATH, dtype=LOAD_TYPES, parse_dates=["date"])
train_all['onpromotion'] = train_all['onpromotion'].fillna(0).astype(int)
test['onpromotion'] = test['onpromotion'].fillna(0).astype(int)

holidays_events = pd.read_csv(HOLIDAY_DATA_PATH, parse_dates=["date"]).rename(columns = {'type': 'holiday_type'})
oil = pd.read_csv(OIL_DATA_PATH, parse_dates=["date"]).rename(columns = {'dcoilwtico': 'oil_price'})
items = pd.read_csv(ITEMS_DATA_PATH)
stores = pd.read_csv(STORES_DATA_PATH)
print('{}: Finish reading data'.format(current_time()))

print('{}: Start create observations for unseen items'.format(current_time()))
promo_train = train_all.set_index(["store_nbr", "item_nbr", "date"])[["onpromotion"]].unstack(level=-1).fillna(0)
promo_train.columns = promo_train.columns.get_level_values(1)

promo_test = test.set_index(["store_nbr", "item_nbr", "date"])[["onpromotion"]].unstack(level=-1).fillna(0)
promo_test.columns = promo_test.columns.get_level_values(1)
promo_test = promo_test.reindex(promo_train.index).fillna(0) # By default, values in the new index that do not have corresponding records in the dataframe are assigned NaN.
promo_all = pd.concat([promo_train, promo_test], axis=1)
del promo_train, promo_test

sales = train_all.set_index(["store_nbr", "item_nbr", "date"])[["unit_sales"]].unstack(level=-1).fillna(0)
sales.columns = sales.columns.get_level_values(1)
print('{}: Finsh creating observations for unseen items'.format(current_time()))

""" Date Range Constant """
NUM_WEEK_CONCATS = 7
T_TEST_START = test['date'].min()
T_TEST_END = test['date'].max()
print('T_TEST_START: {}'.format(convert_date_to_string(T_TEST_START, dow = True)))
print('T_TEST_END: {}'.format(convert_date_to_string(T_TEST_END, dow = True)))
T_VAL_START = T_TEST_START - timedelta(days=7 * 3)
T_VAL_END = T_TEST_END - timedelta(days=7 * 3)
print('T_VAL_START: {}'.format(convert_date_to_string(T_VAL_START, dow = True)))
print('T_VAL_END: {}'.format(convert_date_to_string(T_VAL_END, dow = True)))
T_TRAIN_CONCAT_START_LAST = T_VAL_START - timedelta(days=7 * 3)
T_TRAIN_CONCAT_END_LAST = T_VAL_END - timedelta(days=7 * 3)
T_TRAIN_CONCAT_START_FIRST = T_TRAIN_CONCAT_START_LAST - timedelta(days=7 * (NUM_WEEK_CONCATS-1))
T_TRAIN_CONCAT_END_FIRST = T_TRAIN_CONCAT_END_LAST - timedelta(days=7 * (NUM_WEEK_CONCATS-1))
print('T_TRAIN_CONCAT_START_FIRST: {}'.format(convert_date_to_string(T_TRAIN_CONCAT_START_FIRST, dow = True)))
print('T_TRAIN_CONCAT_END_FIRST: {}'.format(convert_date_to_string(T_TRAIN_CONCAT_END_FIRST, dow = True)))
print('T_TRAIN_CONCAT_START_LAST: {}'.format(convert_date_to_string(T_TRAIN_CONCAT_START_LAST, dow = True)))
print('T_TRAIN_CONCAT_END_LAST: {}'.format(convert_date_to_string(T_TRAIN_CONCAT_END_LAST, dow = True)))


""" Transform Holiday """
def transform_holidays_events(holidays_events):
    HOLIDAY_START = pd.Timestamp(2016, 6, 30)
    HOLIDAY_END = pd.Timestamp(2017, 12, 31)

    holidays_events = holidays_events[(holidays_events['date'] >= HOLIDAY_START) & (holidays_events['date'] <= HOLIDAY_END)]

    # holidays_events.head()
    # Remove Traslado
    # holidays_events[holidays_events['description'].str.contains('Traslado')]
    holidays_events['description'] = holidays_events['description'].str.split('Traslado ').str.get(-1)

    # Split to Local/Regional/National
    # holidays_events['locale'].value_counts()
    local_holiday = holidays_events[holidays_events['locale'] == 'Local']
    regional_holiday = holidays_events[holidays_events['locale'] == 'Regional']
    national_holiday = holidays_events[holidays_events['locale'] == 'National']

    # Remove location
    # local_holiday['description'].value_counts()
    # local_holiday[~local_holiday['description'].str.contains('de')]
    local_holiday['description'] = local_holiday['description'].str.split(' de').str.get(0)
    # local_holiday['description'].value_counts()

    state_city = stores[['state', 'city']].drop_duplicates()
    regional_holiday = regional_holiday.merge(state_city, left_on = 'locale_name', right_on = 'state')
    regional_holiday = regional_holiday.drop(columns = ['locale_name', 'state']).rename(columns = {'city': 'locale_name'})
    regional_holiday['description'] = regional_holiday['description'].str.split(' ').str.get(0)

    # national_holiday.sort_values(['locale_name']).locale_name.unique()
    # national_holiday.sort_values(['description']).description.unique()
    nation_city = stores[['city']].drop_duplicates()
    nation_city['nation'] = 'Ecuador'
    national_holiday = national_holiday.merge(nation_city, left_on = 'locale_name', right_on = 'nation')
    national_holiday = national_holiday.drop(columns = ['locale_name', 'nation']).rename(columns = {'city': 'locale_name'})

    # Concatenate Local/Regional/National holidays
    holidays_events = pd.concat([local_holiday, regional_holiday, national_holiday]).rename(columns = {'locale_name': 'city'})
    holidays_events_onehot_encoded = pd.get_dummies(holidays_events, columns = ['description', 'holiday_type', 'locale'])
    holidays_events_onehot_encoded['transferred'] = holidays_events_onehot_encoded['transferred'].astype(int)

    # Some city has several holidays in one day
    # holidays_events = holidays_events.sort_values(['date'])
    # holidays_events[holidays_events.duplicated(subset = ['city', 'date'], keep = False)]
    holidays_events = holidays_events_onehot_encoded.groupby(['date', 'city']).max().reset_index()
    holidays_events = holidays_events.merge(stores[['city', 'store_nbr']]).drop(columns = ['city'])

    # Only indicate the description that appear in the testset
    col_unique_df = holidays_events[(holidays_events['date'] >= T_TEST_START) & (holidays_events['date'] <= T_TEST_END)].nunique().reset_index()
    columns_to_kept = list(set(list(col_unique_df[col_unique_df[0] > 1]['index']) + ['date']))
    holidays_events = holidays_events[columns_to_kept].drop_duplicates()

    store_holidays = []
    for store_nbr in tqdm(list(stores['store_nbr'].unique())):
        store_holiday = holidays_events[holidays_events['store_nbr'] == store_nbr]
        holiday_dates = list(store_holiday['date'])
        store_holiday['is_holiday'] = 1
        store_holiday = store_holiday.set_index(['date']).reindex(pd.date_range(train_all['date'].min(), pd.Timestamp(2017, 8, 31))).fillna(0).reset_index().rename(columns = {'index': 'date'})
        store_holiday['store_nbr'] = store_nbr

        def distance_to_next_holiday(date):
            distance = 365
            for holiday_date in holiday_dates:
                days = (holiday_date - date).days
                if days >= 0:
                    distance = min(distance, days)
            return distance

        def distance_from_prev_holiday(date):
            distance = 365
            for holiday_date in holiday_dates:
                days = (date - holiday_date).days
                if days >= 0:
                    distance = min(distance, days)
            return distance

        holiday_df = pd.DataFrame(data = {'date' : pd.date_range(HOLIDAY_START, HOLIDAY_END)})
        holiday_df['days_to_next_holiday'] = holiday_df['date'].apply(distance_to_next_holiday)
        holiday_df['days_from_prev_holiday'] = holiday_df['date'].apply(distance_from_prev_holiday)
        store_holiday = store_holiday.merge(holiday_df)
        store_holidays.append(store_holiday)

    store_holidays = pd.concat(store_holidays)

    return store_holidays
store_holidays = transform_holidays_events(holidays_events)
store_holidays = store_holidays.set_index(['store_nbr', 'date']).unstack(level=-1).swaplevel(0, 1, axis=1)
store_holidays = store_holidays.reindex(sales.index.get_level_values(0))
store_holidays_columns = [col for col in list(store_holidays.columns.get_level_values(1).unique()) if col not in ['date', 'store_nbr']]

""" Oil Price """
def transform_oil_price(oil):
    # Add rows for weekends
    oil = oil.set_index(['date']).reindex(pd.date_range(oil['date'].min(), oil['date'].max()))
    # fill missing oil price with the average of previous and after days
    oil = oil.where(~pd.isna(oil['oil_price']), other=(oil['oil_price'].fillna(method='ffill') + oil['oil_price'].fillna(method='bfill'))/2, axis = 0).fillna(method='bfill').reset_index().rename(columns = {'index': 'date'})
    oil = oil.set_index('date').transpose()
    return oil

oil = transform_oil_price(oil)

"""" Pay Day """
def get_pay_days():

    PAY_DAY_START = pd.Timestamp(2016, 12, 1)
    PAY_DAY_END = pd.Timestamp(2017, 8, 31)

    def distance_to_next_payday(date):
        distance = 365
        for pay_day in pay_days:
            days = (pay_day - date).days
            if days >= 0:
                distance = min(distance, days)
        return distance

    def distance_from_prev_payday(date):
        distance = 365
        for pay_day in pay_days:
            days = (date - pay_day).days
            if days >= 0:
                distance = min(distance, days)
        return distance

    days_df = pd.DataFrame(data = {'date': pd.date_range(PAY_DAY_START, PAY_DAY_END)})
    days_df['day'] = days_df['date'].dt.day
    days_df['month'] = days_df['date'].dt.month
    days_15 = list(days_df[days_df['day'] == 15]['date'])
    days_end = list(days_df.drop_duplicates(subset = ['month'], keep = 'last')['date'])
    pay_days = days_15 + days_end
    days_df['days_to_next_pay_day'] = days_df['date'].apply(distance_to_next_payday)
    days_df['days_from_prev_pay_day'] = days_df['date'].apply(distance_from_prev_payday)
    days_df = days_df.set_index('date')[['days_to_next_pay_day', 'days_from_prev_pay_day']].transpose()

    return days_df.loc['days_to_next_pay_day'], days_df.loc['days_from_prev_pay_day']

days_to_next_pay_day, days_from_prev_pay_day = get_pay_days()

""" Mean Encoding
"""
train_sales_mean_encoded = sales.stack(level=-1).reset_index().rename(columns = {0: 'unit_sales'})
# train_sales_mean_encoded = train_all.set_index(["store_nbr", "item_nbr", "date"])[["unit_sales"]].unstack(level=-1).fillna(0).stack(level=-1).reset_index()
train_sales_mean_encoded = train_sales_mean_encoded.merge(items).merge(stores).sort_values(['date'])
test_sales_mean_encoded = test.set_index(["store_nbr", "item_nbr", "date"]).unstack(level=-1).fillna(0).stack(level=-1).reset_index()
test_sales_mean_encoded = test_sales_mean_encoded.merge(items).merge(stores).sort_values(['date'])
# pd.concat([train_sales_mean_encoded, train_sales_mean_encoded[['item_nbr', 'unit_sales']].groupby('item_nbr').cumsum()], axis = 1).sort_values(['item_nbr', 'date'])

Target = 'unit_sales'
y_tr = train_sales_mean_encoded[Target].values
corrcoefs_dict = {}

# item_col = ['item_nbr', 'family', 'class']
# store_col = ['store_nbr', 'city', 'state', 'type', 'cluster']
# item_store_pairs = [pair for pair in list(product(item_col, store_col))]
# mean_encoded_columns = item_col + store_col + item_store_pairs
mean_encoded_columns = ['item_nbr', 'family', 'class', 'store_nbr']


print('=' * 50)
print('{}: Start adding mean-encoding for {} by {}'.format(current_time(), Target, mean_encoded_columns))
for col in tqdm(mean_encoded_columns):

    print('{}: Start calculating mean-encoding for {} by {}'.format(current_time(), Target, col))
    corrcoefs = pd.DataFrame(columns = ['Column', 'Scheme', 'Cor'])

    if type(col) is str:
        global_mean =  train_sales_mean_encoded.groupby(col)[Target].mean().mean()
        col_tr = train_sales_mean_encoded[[col] + [Target]]

        target_sum = col_tr.groupby(col)[Target].sum()
        target_count = col_tr.groupby(col)[Target].count()
        col_tr[col + '_' + Target + '_sum'] = col_tr[col].map(target_sum)
        col_tr[col + '_' + Target + '_count'] = col_tr[col].map(target_count)
        col_tr[col + '_' + Target + '_mean_LOO'] = (col_tr[col + '_' + Target + '_sum'] - col_tr[Target]) / (col_tr[col + '_' + Target + '_count'] - 1)
        col_tr.fillna(global_mean, inplace = True)
        corrcoefs.loc[col + '_' + Target + '_mean_LOO'] = [col, 'LOO', np.corrcoef(y_tr, col_tr[col + '_' + Target + '_mean_LOO'])[0][1]]

    else:
        col_str = col[0] + '&' + col[1]
        col_list = list(col)

        global_mean =  train_sales_mean_encoded.groupby(col)[Target].mean().mean()
        col_tr = train_sales_mean_encoded[col_list + [Target]]

        target_sum = col_tr.groupby(col_list)[Target].sum().reset_index().rename(columns = {Target: 'Sum'})
        target_count = col_tr.groupby(col_list)[Target].count().reset_index().rename(columns = {Target: 'Count'})
        df = col_tr.merge(target_sum).merge(target_count)
        col_tr[col_str + '_' + Target + '_mean_LOO'] = (df['Sum'] - df[Target]) / (df['Count'] - 1)
        col_tr.fillna(global_mean, inplace = True)
        corrcoefs.loc[col_str + '_' + Target + '_mean_LOO'] = [col_str, 'LOO', np.corrcoef(y_tr, col_tr[col_str + '_' + Target + '_mean_LOO'])[0][1]]

    print(corrcoefs.sort_values('Cor'))
    if corrcoefs['Cor'].max() >= 0.25:
        train_sales_mean_encoded = pd.concat([train_sales_mean_encoded, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)
    corrcoefs_dict[col] = corrcoefs

    print('{}: Finish encoding for {} by'.format(current_time(), Target, col))

print('{}: Finish adding mean-encoding for {}'.format(current_time(), Target))

corrcoefs_all = pd.concat(list(corrcoefs_dict.values()))
# corrcoefs_all.to_csv('results/mean_encoding_corrcoefs.csv', index = False)
corrcoefs_all.sort_values('Cor', ascending = False)

mean_columns = [col for col in train_sales_mean_encoded.columns if 'unit_sales_mean' in col]
print('-' * 50)
print('Mean Encoded Columns:')
for col in mean_columns:
    print(col.split('_unit_sales')[0])

print('{}: Start adding mean-encoding for test'.format(current_time()))
for mean_col in mean_columns:
    col = mean_col.split('_unit_sales_mean_')[0]

    if '&' not in col:
        global_mean =  train_sales_mean_encoded.groupby(col)[Target].mean().mean()
        col_tr = train_sales_mean_encoded[[col] + [Target]]
        target_mean = col_tr.groupby(col)[Target].mean()

        test_sales_mean_encoded[mean_col] = test_sales_mean_encoded[col].map(target_mean)
        test_sales_mean_encoded[mean_col].fillna(global_mean, inplace = True)
    else:
        col_list = col.split('&')
        global_mean =  train_sales_mean_encoded.groupby(col_list)[Target].mean().mean()
        col_tr = train_sales_mean_encoded[col_list + [Target]]
        target_mean = col_tr.groupby(col_list)[Target].mean().reset_index().rename(columns = {Target: mean_col})

        test_sales_mean_encoded = test_sales_mean_encoded.merge(target_mean)
print('{}: Finish adding mean-encoding for test'.format(current_time()))

combined = pd.concat([train_sales_mean_encoded, test_sales_mean_encoded])
mean_encoding_all = combined.set_index(["store_nbr", "item_nbr", "date"])[mean_columns].unstack(level=-1).fillna(0)
del combined, train_sales_mean_encoded, test_sales_mean_encoded


""" Lag Features """
items = items.set_index("item_nbr").reindex(sales.index.get_level_values(1)).reset_index()
stores = stores.set_index("store_nbr").reindex(sales.index.get_level_values(0)).reset_index()

def get_timespan(df, dt, minus, periods, freq='D'):
    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]

def prepare_subdataset(sales_df, promo_df, t_cur, is_train=True, name_prefix=None):
    X = {}

    for i in range(7):
        # Specific Feature: Average sales for same dayofweek for the last few weeks
        for j in [1, 2, 3]:
            X['{}sales_prev_{}_dow'.format(''.join(['DAY{}_'.format(day) for day in list(range(i, 16, 7))]), j)] = sales_df[t_cur - timedelta(days= j * 7-i)].values.astype(np.uint8)

        for j in [4, 12, 20]:
            X['{}sales_prev_{}_dow_mean'.format(''.join(['DAY{}_'.format(day) for day in list(range(i, 16, 7))]), j)] = get_timespan(sales_df, t_cur, j * 7-i, j, freq='7D').mean(axis=1).values

    # General Feature of previous days of sales/promo
    for i in [1, 2, 3, 7, 14, 30, 60, 120]:

        # Sales mean for past periods
        X["sales_prev_{}_mean".format(i)]= get_timespan(sales_df, t_cur,i, i).mean(axis=1).values

        # Mean sales for days with/out promotion for past periods
        tmp1 = get_timespan(sales_df, t_cur, i, i)
        tmp2 = (get_timespan(promo_df, t_cur, i, i) > 0) * 1
        X['sales_has_promo_mean_{}'.format(i)] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).fillna(0).values
        X['sales_no_promo_mean_{}'.format(i)] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).fillna(0).values

        # promo count for past periods
        X['promo_prev_{}_count'.format(i)] = get_timespan(promo_df, t_cur, i, i).sum(axis=1).values.astype(np.uint8)

        # Days to first/last sale
        tmp = get_timespan(sales_df, t_cur, i, i)
        X['last_has_sales_day_in_last_{}'.format(i)] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values
        X['first_has_sales_day_in_last_{}'.format(i)] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values

        # Days to first/last promo
        tmp = get_timespan(promo_df, t_cur, i, i)
        X['last_has_promo_day_in_last_{}'.format(i)] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values
        X['first_has_promo_day_in_last_{}'.format(i)] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values

    for i in range(16):

        # Specific feature: Has promo for this day
        X["DAY{}_promo".format(i)] = promo_df[t_cur + timedelta(days=i)].values.astype(np.uint8)

        for col in store_holidays_columns:
            X['DAY{}_holiday_{}'.format(i, col)] = store_holidays[t_cur][col].values


        # Oil price movement for past 3 days
        X['DAY{}_oil_price_diff_day1'.format(i)] = np.array([get_timespan(oil, t_cur+timedelta(days = i), 1, 2).diff(axis=1).iloc[-1, -1]] * promo_all.shape[0])
        X['DAY{}_oil_price_diff_day2'.format(i)] = np.array([get_timespan(oil, t_cur+timedelta(days = i), 2, 2).diff(axis=1).iloc[-1, -1]] * promo_all.shape[0])
        X['DAY{}_oil_price_diff_day3'.format(i)] = np.array([get_timespan(oil, t_cur+timedelta(days = i), 3, 2).diff(axis=1).iloc[-1, -1]] * promo_all.shape[0])

        # Oil price difference between recent week and previous week
        X['DAY{}_oil_price_diff_week'.format(i)] = np.array(list(get_timespan(oil, t_cur+timedelta(days = i), 6, 7).mean(axis=1) - get_timespan(oil, t_cur+timedelta(days = i), 13, 7).mean(axis=1)) * promo_all.shape[0])

        # Oil price difference between recent month and previous month
        X['DAY{}_oil_price_diff_month'.format(i)] = np.array(list(get_timespan(oil, t_cur+timedelta(days = i), 29, 30).mean(axis=1) - get_timespan(oil, t_cur+timedelta(days = i), 59, 30).mean(axis=1)) * promo_all.shape[0])

        # Days_to_previous/next_Payday
        X['DAY{}_days_to_next_pay_day'.format(i)] = np.array([days_to_next_pay_day[t_cur+timedelta(days = i)]] * promo_all.shape[0])
        X['DAY{}_days_from_prev_pay_day'.format(i)] = np.array([days_from_prev_pay_day[t_cur+timedelta(days = i)]] * promo_all.shape[0])

    X = pd.DataFrame(X)

    if is_train:
        y = sales_df[pd.date_range(t_cur, periods=16)].values
        return X, y
    if name_prefix is not None:
        X.columns = ['{}_{}'.format(name_prefix, c) for c in X.columns]
    return X

def prepare_dataset(t_cur, is_train=True):
    print('Start preparing dataset for periods {} to {}'.format(t_cur.date(),(t_cur + timedelta(days = 15)).date()))
    if is_train:
        X_tmp1, y_tmp = prepare_subdataset(sales, promo_all, t_cur, is_train=True)
    else:
        X_tmp1 = prepare_subdataset(sales, promo_all, t_cur, is_train=False)

    X_mean_encoded = {}
    for col in mean_columns:
        mean_encoding = mean_encoding_all[col][[t_cur]]
        mean_encoding = mean_encoding.reindex(sales.index).fillna(0).reset_index(drop=True)
        X_mean_encoded[col] = mean_encoding.values.ravel()
    X_mean_encoded = pd.DataFrame(X_mean_encoded)
    X_tmp = pd.concat([X_tmp1, items, stores, X_mean_encoded], axis=1)
    del X_mean_encoded

    del X_tmp1
    gc.collect()

    print('Finish preparing dataset for periods {} to {}'.format(t_cur.date(),(t_cur + timedelta(days = 15)).date()))

    if is_train:
        return X_tmp, y_tmp
    else:
        return X_tmp

print('=' * 50)
print('{}: Start preparing lag features for {} Weeks of Concats'.format(current_time(), NUM_WEEK_CONCATS))
X_l, y_l = [], []
t_cur = T_TRAIN_CONCAT_START_FIRST
for i in tqdm(range(NUM_WEEK_CONCATS)):

    delta = timedelta(days=7 * i)
    X_tmp, y_tmp = prepare_dataset(T_TRAIN_CONCAT_START_FIRST + delta, is_train=True)
    X_l.append(X_tmp)
    y_l.append(y_tmp)

    del X_tmp
    gc.collect()

X_train = pd.concat(X_l, axis=0)
y_train = np.concatenate(y_l, axis=0)
del X_l, y_l
gc.collect()

X_val, y_val = prepare_dataset(T_VAL_START, is_train=True)
X_test = prepare_dataset(T_TEST_START, is_train=False)
print('{}: Finish preparing lag features'.format(current_time()))

# Save y_val for stacking later
y_val_df = pd.DataFrame(y_val, index=sales.index, columns=pd.date_range(T_VAL_START, periods=16)).stack().to_frame("unit_sales")
y_val_df.index.set_names(["store_nbr", "item_nbr", "date"], inplace=True)
y_val_df["unit_sales"] = np.clip(np.expm1(y_val_df["unit_sales"]), 0, 1000)
y_val_df.reset_index().to_csv('results/y_val.csv', index = False)

print('Shape of X_train: {}'.format(X_train.shape))
print('Shape of X_val: {}'.format(X_val.shape))
print('Shape of X_test: {}'.format(X_test.shape))

def get_specific_columns(x_train):
    return [col for col in x_train.columns if 'DAY' in col]

def get_specific_columns_day_n(x_train, n):
    return [col for col in get_specific_columns(x_train) if 'DAY{}_'.format(n) in col]

def get_general_columns(x_train):
    return [col for col in x_train.columns if 'DAY' not in col]

def get_continuous_columns(x_train):
    return [col for col in X_train.columns if col not in categorical_features]

print('Length of general columns: {}'.format(len(get_general_columns(X_train))))
print('Length of specific columns: {}'.format(len(get_specific_columns(X_train))))
print('Length of specific columns per day: {}'.format(len(get_specific_columns_day_n(X_train, 0))))
print('Length of total columns per day: {}'.format(len(get_general_columns(X_train)) + len(get_specific_columns_day_n(X_train, 0))))
# X_val.columns[X_val.isna().any()].tolist()
# X_train.describe().max()

""" Normalization """
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
categorical_features = ['item_nbr', 'family', 'class', 'store_nbr', 'city', 'state', 'type', 'cluster', 'perishable']
X_train[get_continuous_columns(X_train)] = scaler.fit_transform(X_train[get_continuous_columns(X_train)])
X_val[get_continuous_columns(X_val)] = scaler.transform(X_val[get_continuous_columns(X_val)])
X_test[get_continuous_columns(X_test)] = scaler.transform(X_test[get_continuous_columns(X_test)])

""" Categorical Columns """
# LabelEncoder categories
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in ['family', 'city', 'state', 'type']:
    for data in [X_train, X_val, X_test]:
        if col in data.columns:
            data[col] = le.fit_transform(data[col].values)

""" Result Processing Functions """
from sklearn.metrics import mean_squared_error
def get_rmse(y_true, y_test):
    return mean_squared_error(y_true, y_test) ** 0.5

def calculate_nwrmsle(y_true, y_pred, num_week):
    weight = pd.concat([items["perishable"]] * num_week) * 0.25 + 1
    err = (y_true - np.array(y_pred).transpose())**2
    err_all = np.sqrt((err.sum(axis=1) * weight).sum() / weight.sum() / 16)
    err_by_day = pd.DataFrame({i: [np.sqrt((err[:, i] * weight).sum() / weight.sum())] for i in range(16)}).transpose()
    err_by_day.loc['All'] = err_all
    return err_by_day

def process_nwrmsle(y_train, train_pred, y_val, val_pred, save_path = None):
    train_nwrmsle = calculate_nwrmsle(y_train, train_pred, num_week = NUM_WEEK_CONCATS).rename(columns = {0: 'Train NWRMSLE'})
    val_nwrmsle = calculate_nwrmsle(y_val, val_pred, num_week = 1).rename(columns = {0: 'Val NWRMSLE'})
    nwrmsle = pd.concat([train_nwrmsle, val_nwrmsle], axis = 1)
    if save_path:
        nwrmsle.to_csv(save_path, index = False)
    return nwrmsle

def prepare_preds_df(y_pred, t_start):
    pred_df = pd.DataFrame(np.array(y_pred).transpose(), index=sales.index, columns=pd.date_range(t_start, periods=16)).stack().to_frame("unit_sales")
    pred_df.index.set_names(["store_nbr", "item_nbr", "date"], inplace=True)
    pred_df["unit_sales"] = np.clip(np.expm1(pred_df["unit_sales"]), 0, 1000)
    return pred_df

def save_pred_df(y_pred, pred_type, path):
    if pred_type == 'val':
        prepare_preds_df(y_pred, T_VAL_START).reset_index().to_csv(path, index = False)
        print('Saved {} prediction to {}'.format(pred_type, path))
    elif pred_type == 'test':
        prepare_preds_df(y_pred, T_TEST_START).reset_index().to_csv(path, index = False)
        print('Saved {} prediction to {}'.format(pred_type, path))
    else:
        print('WARNING: pred_type must be test or val')

def make_submission(test_pred, t_start, path):
    pred_df = prepare_preds_df(test_pred, t_start)
    submission = test.set_index(["store_nbr", "item_nbr", "date"])[["id"]].join(pred_df, how="left").fillna(0)
    submission.to_csv(path, index = False)
    print('{}: Saved submission to {}'.format(current_time(), path))

def save_importances(importances, path):
    importances_df = pd.DataFrame(importances)
    importances_df['Mean'] = importances_df.iloc[:, 1:].abs().mean(axis = 1)
    importances_df.to_csv(path, index = False)
    print('{}: Saved importances to {}'.format(current_time(), path))

def save_settings(estimator_name, settings, stacked):
    print('=' * 50)
    print('SETTINGS:')
    settings = pd.DataFrame(data = settings, index = range(1)).transpose()
    print(settings)
    settings.to_csv(convert_filename('results/settings-{}-{}{}'.format(estimator_name, start_time, stacked)) + '.csv')

def process_result(estimator_name, y_train, train_pred, y_val, val_pred, test_pred, T_TEST_START, importances, settings, stacked):
    save_settings(estimator_name, settings, stacked)
    print('-' * 50)
    print('RESULTS:')
    print(process_nwrmsle(y_train, train_pred, y_val, val_pred, save_path = convert_filename('results/nwrmsle-{}-{}'.format(estimator_name, start_time)) + '.csv'))
    save_pred_df(val_pred, 'val', convert_filename('results/pred_{}-{}-{}'.format('val', estimator_name, start_time)) + '.csv')
    save_pred_df(test_pred, 'test', convert_filename('results/pred_{}-{}-{}'.format('test', estimator_name, start_time)) + '.csv')
    make_submission(test_pred, T_TEST_START, convert_filename('submission/sub-{}-{}'.format(estimator_name, start_time)) + '.csv')
    save_importances(importances, convert_filename('results/feature_importances-{}-{}'.format(estimator_name, start_time)) + '.csv')

def process_rfecv_reult(rfecv_selector, columns, day):
    rfecv_result = pd.DataFrame(data = {'Ranking': rfecv_selector.ranking_, 'Column': columns}).sort_values('Ranking')
    rfecv_result['RMSE'] = np.sqrt(-1 * rfecv_selector.grid_scores_)
    rfecv_result = rfecv_result.reset_index().drop(columns = ['index'])
    rfecv_result.to_csv('results/LinearRegression-rfecv_result-Day{}-{}.csv'.format(day, convert_filename(start_time)), index = False)

def select_columns(selector, x):
    cols = x.columns[selector.support_]
    x = pd.DataFrame(selector.transform(x))
    x.columns = cols
    return x

def set_data(x_train, x_val, x_test, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False):
    columns_to_drop = []
    if not add_mean_encoding:
        columns_to_drop = columns_to_drop + [col for col in x_train.columns if col in mean_columns]
    if not add_holiday:
        columns_to_drop = columns_to_drop + [col for col in x_train.columns if 'holiday' in col]
    if not add_oil:
        columns_to_drop = columns_to_drop + [col for col in x_train.columns if 'oil' in col]
    if not add_pay_day:
        columns_to_drop = columns_to_drop + [col for col in x_train.columns if 'pay_day' in col]
    x_train = x_train.drop(columns = columns_to_drop)
    x_val = x_val.drop(columns = columns_to_drop)
    x_test = x_test.drop(columns = columns_to_drop)
    return x_train, x_val, x_test

def create_model(x_train):
    model = Sequential()
    model.add(Dense(512, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(256, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(64, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(32, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(16, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(8, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Dense(1, activation='linear'))

    model.compile(loss='mse', optimizer='adam', metrics=['mse'])

    return model

def train_model(estimator, x_train, y_train, x_val, y_val, x_test, \
    add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False, \
    lgb_cat = False, lr_full = False, tree_full = False, stacked = ''):

    x_train, x_val, x_test = set_data(x_train, x_val, x_test, add_mean_encoding, add_holiday, add_oil, add_pay_day)

    if estimator == 'NeuralNetwork':
        estimator_name = 'NeuralNetwork'
    else:
        try:
            estimator_name = estimator().__class__.__name__
        except:
            estimator_name = estimator.__name__

    print('=' * 50)
    print('{}: Start Training {}'.format(current_time(), estimator_name))
    runtime_df = {}
    val_pred = []
    test_pred = []
    train_pred = []
    importances = {}
    if use_specific_columns:
        importances['Columns'] = [re.sub('DAY\d+_', 'DAYN_', col) for col in get_general_columns(x_train) + get_specific_columns_day_n(x_train, 0)]
    else:
        importances['Columns'] = x_train.columns

    for i in range(16):
        print("-" * 50)
        print('{}: Start Training for Day {}'.format(current_time(), i))

        if use_specific_columns:
            selected_columns = get_general_columns(x_train) + get_specific_columns_day_n(x_train, i)
            x_train_selected, x_val_selected, x_test_selected = x_train[selected_columns], x_val[selected_columns], x_test[selected_columns]
        else:
            x_train_selected, x_val_selected, x_test_selected = x_train.copy(), x_val.copy(), x_test.copy()

        if estimator_name is 'lightgbm':
            MAX_ROUNDS = 1000
            dtrain = lgb.Dataset(x_train_selected, label=y_train[:, i],
                weight=pd.concat([items["perishable"]] * NUM_WEEK_CONCATS) * 0.25 + 1)
            dval = lgb.Dataset(x_val_selected, label=y_val[:, i], reference=dtrain,
                weight=items["perishable"] * 0.25 + 1)
            start = time.perf_counter()
            if lgb_cat:
                model = lgb.train(estimatos_params_grid[estimator_name], dtrain, num_boost_round=MAX_ROUNDS,
                    valid_sets=[dtrain, dval], early_stopping_rounds=50, verbose_eval=50,
                    categorical_feature = categorical_features)
            else:
                model = lgb.train(estimatos_params_grid[estimator_name], dtrain, num_boost_round=MAX_ROUNDS,
                    valid_sets=[dtrain, dval], early_stopping_rounds=50, verbose_eval=50,
                    categorical_feature = [])
            runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start

            y_pred_train = model.predict(x_train_selected, num_iteration=model.best_iteration or MAX_ROUNDS)
            y_pred_val = model.predict(x_val_selected, num_iteration=model.best_iteration or MAX_ROUNDS)
            y_pred_test = model.predict(x_test_selected, num_iteration=model.best_iteration or MAX_ROUNDS)

            importances['Day{}'.format(i)] = model.feature_importance("gain")

        elif estimator_name is 'NeuralNetwork':
            y_train_single_day = y_train[:, i]
            y_val_single_day = y_val[:, i]
            model = create_model(x_train_selected)

            checkpoint_path = convert_filename("results/weights-{}-Day{}".format(start_time, i)) + '.ckpt'
            checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=0, save_best_only = True)
            earlystopping = tf.keras.callbacks.EarlyStopping(patience = 8, verbose=1, monitor='val_mean_squared_error')
            tensorboard = tf.keras.callbacks.TensorBoard(log_dir=convert_filename('results/log-{}-Day{}/logs'.format(start_time, i)))

            sample_weights=np.array( pd.concat([items["perishable"]] * NUM_WEEK_CONCATS) * 0.25 + 1 )
            start = time.perf_counter()
            history = model.fit(x_train_selected, y_train_single_day, epochs = 1000, batch_size=32, validation_data = (x_val_selected, y_val_single_day),
                                callbacks = [checkpoint, earlystopping, tensorboard], sample_weight = sample_weights)
            runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start
            model.load_weights(checkpoint_path)

            y_pred_train = model.predict(x_train_selected).ravel()
            y_pred_val = model.predict(x_val_selected).ravel()
            y_pred_test = model.predict(x_test_selected).ravel()

        elif estimator_name is 'LinearRegression':
            y_train_single_day = y_train[:, i]
            y_val_single_day = y_val[:, i]

            if lr_full:
                model = estimator(**estimatos_params_grid[estimator_name])
                model.fit(x_train_selected, y_train_single_day)
                y_pred_train = model.predict(x_train_selected)
                y_pred_val = model.predict(x_val_selected)

                model = estimator(**estimatos_params_grid[estimator_name])
                start = time.perf_counter()
                model.fit(x_train_selected.append(x_val_selected), np.concatenate([y_train_single_day, y_val_single_day]))
                runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start

                y_pred_test = model.predict(x_test_selected)
                importances['Day{}'.format(i)] = model.coef_

            else:
                print('{}: Start RFECV for {}'.format(current_time(), estimator_name))
                lr_selector = RFECV(LinearRegression(), step=1, cv=3, verbose=0, n_jobs = -1, scoring='neg_mean_squared_error')
                lr_selector = lr_selector.fit(x_train_selected.append(x_val_selected), np.concatenate([y_train_single_day, y_val_single_day]))

                process_rfecv_reult(lr_selector, x_train_selected.columns, i)

                model = estimator(**estimatos_params_grid[estimator_name])
                model.fit(select_columns(lr_selector, x_train_selected), y_train_single_day)
                y_pred_train = model.predict(select_columns(lr_selector, x_train_selected))
                y_pred_val = model.predict(select_columns(lr_selector, x_val_selected))

                model = estimator(**estimatos_params_grid[estimator_name])
                start = time.perf_counter()
                model.fit(select_columns(lr_selector, x_train_selected.append(x_val_selected)), np.concatenate([y_train_single_day, y_val_single_day]))
                runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start

                y_pred_test = model.predict(select_columns(lr_selector, x_test_selected))

                columns = select_columns(lr_selector, x_train_selected).columns
                columns = [re.sub('DAY\d+_', 'DAYN_', col) for col in columns]
                coefs = pd.DataFrame(dict(zip(columns, model.coef_)), index = [0]).transpose().rename(columns = {0: 'Coef'}).reindex(importances['Columns']).fillna(0)
                importances['Day{}'.format(i)] = coefs['Coef']

        else:
            y_train_single_day = y_train[:, i]
            y_val_single_day = y_val[:, i]

            if tree_full:
                model = estimator(random_state = SEED)
                model.fit(x_train_selected, y_train_single_day)
                y_pred_train = model.predict(x_train_selected)
                y_pred_val = model.predict(x_val_selected)

                model = estimator(random_state = SEED)
                start = time.perf_counter()
                model.fit(x_train_selected.append(x_val_selected), np.concatenate([y_train_single_day, y_val_single_day]))
                runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start

                y_pred_test = model.predict(x_test_selected)
                importances['Day{}'.format(i)] = model.feature_importances_

            else:
                print(current_time() + ': Start grid searching for ' + estimator_name)
                gridsearcher = GridSearch(model = estimator(), param_grid = estimatos_params_grid[estimator_name])
                gridsearch_param = {'scoring': 'neg_mean_squared_error', 'verbose': 2 }
                model = gridsearcher.fit(x_train_selected, y_train_single_day, x_val_selected, y_val_single_day, **gridsearch_param)
                print('Grid Search best RMSE: {}'.format((-1 * gridsearcher.best_score)**0.5))
                save_obj(gridsearcher.best_params, convert_filename('GridSearch-Best_Params-{}-Day{}-{}'.format(estimator_name, i, start_time)))
                save_obj(gridsearcher.best_estimator_.get_params(), convert_filename('Params-{}-Day{}-{}'.format(estimator_name, i, start_time)))
                print(current_time() + ': Finished grid searching for ' + estimator_name)
                print('Best Params: \n{}'.format(gridsearcher.best_params))

                y_pred_train = model.predict(x_train_selected)
                y_pred_val = model.predict(x_val_selected)

                model = estimator(**gridsearcher.best_estimator_.get_params())
                start = time.perf_counter()
                model.fit(x_train_selected.append(x_val_selected), np.concatenate([y_train_single_day, y_val_single_day]))
                runtime_df['Run Time Day{}'.format(i)] = time.perf_counter() - start

                y_pred_test = model.predict(x_test_selected)

                importances['Day{}'.format(i)] = model.feature_importances_

        train_pred.append(y_pred_train)
        val_pred.append(y_pred_val)
        test_pred.append(y_pred_test)

    print('=' * 50)
    print("Validation rmse:", mean_squared_error(
        y_val, np.array(val_pred).transpose()) ** 0.5)

    weight = items["perishable"] * 0.25 + 1
    err = (y_val - np.array(val_pred).transpose())**2
    err = err.sum(axis=1) * weight
    err = np.sqrt(err.sum() / weight.sum() / 16)
    print('nwrmsle = {}'.format(err))

    settings = {'estimator_name': estimator_name,
                'seed': SEED,
                'add_mean_encoding': add_mean_encoding,
                'add_holiday': add_holiday,
                'add_oil': add_oil,
                'add_pay_day': add_pay_day,
                'use_specific_columns': use_specific_columns,
                'lgb_cat': lgb_cat,
                'lr_full': lr_full,
                'tree_full': tree_full}

    save_obj(runtime_df, convert_filename('Run_Time-{}-{}'.format(estimator_name, start_time)))
    process_result(estimator_name, y_train, train_pred, y_val, val_pred, test_pred, T_TEST_START, importances, settings, stacked)


""" Modelling """
estimatos_params_grid = {
    'LinearRegression': {},
    'DecisionTreeRegressor': {'min_samples_split': [2000, 2500, 3000, 3500, 4000], 'random_state': [SEED]},
    'lightgbm': {
        'objective': 'regression',
        'min_child_samples': 200,
        'learning_rate': 0.07,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 3,
        'metric': 'l2_root',
        'num_threads': 4,
        'random_state': SEED
        }}

start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = False, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False)
start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False, stacked = '-stacked')

start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = True, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False)
start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = False, add_holiday = True, add_oil = False, add_pay_day = False, use_specific_columns = False)
start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = False, add_holiday = False, add_oil = True, add_pay_day = False, use_specific_columns = False)
start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = True, use_specific_columns = False)

start_time = current_time()
train_model(LinearRegression, X_train, y_train, X_val, y_val, X_test, lr_full = True, add_mean_encoding = True, add_holiday = True, add_oil = True, add_pay_day = True, use_specific_columns = False)

start_time = current_time()
train_model(DecisionTreeRegressor, X_train, y_train, X_val, y_val, X_test, tree_full = False, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False, stacked = '-stacked')
start_time = current_time()
train_model(DecisionTreeRegressor, X_train, y_train, X_val, y_val, X_test, tree_full = True, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False)

start_time = current_time()
train_model(lgb, X_train, y_train, X_val, y_val, X_test, lgb_cat = False, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False, stacked = '-stacked')
start_time = current_time()
train_model(lgb, X_train, y_train, X_val, y_val, X_test, lgb_cat = True, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = False)

start_time = current_time()
train_model('NeuralNetwork', X_train, y_train, X_val, y_val, X_test, add_mean_encoding = False, add_holiday = False, add_oil = False, add_pay_day = False, use_specific_columns = True, stacked = '-stacked')


""" Meta Model (Second Level Model)"""
setting_filenames = [filename for filename in os.listdir(TRAIN_RESULT_PATH) if (filename.startswith('setting') and filename.endswith('stacked.csv'))]
settings = [filename.split('.')[0].split('-') for filename in setting_filenames]
settings = pd.DataFrame(settings)
settings['Time'] = settings[2].astype(str) + '-'  + settings[3].astype(str) + '-' + settings[4].astype(str)
best_models_start_time = settings['Time']

val_preds_filenames = [file for file in os.listdir(TRAIN_RESULT_PATH) if (any(start_time in file for start_time in best_models_start_time) and file.startswith('pred_val'))]
val_preds = []
for filename in val_preds_filenames:
    estimator_name = filename.replace('pred_val-', '').split('-')[0]
    pred_df = pd.read_csv(os.path.join(TRAIN_RESULT_PATH, filename), converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0})[['unit_sales']].rename(columns = {'unit_sales': estimator_name})
    val_preds.append(pred_df)

test_preds = []
for filename in val_preds_filenames:
    estimator_name = filename.replace('pred_val-', '').split('-')[0]
    pred_df = pd.read_csv(os.path.join('submission', filename.replace('pred_val', 'sub')), converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0})[['unit_sales']].rename(columns = {'unit_sales': estimator_name})
    test_preds.append(pred_df)

val_preds = pd.concat(val_preds, axis = 1)
test_preds = pd.concat(test_preds, axis = 1)
y_val_real = pd.read_csv('results/y_val.csv', converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0})['unit_sales']

def train_meta(estimator, x_train, y_train, x_test):
    estimator_name = estimator.__class__.__name__
    print('Training Second level learning model via ' + estimator_name)

    start_time = current_time()
    estimator.fit(x_train, y_train)
    print('Validation RMSE: {}'.format(get_rmse(estimator.predict(x_train), y_train)))
    y_test_pred_stacked = np.clip(np.expm1(estimator.predict(x_test)), 0, 1000)
    submission_df = pd.DataFrame(data = {'id': test.set_index(["store_nbr", "item_nbr", "date"])["id"], 'unit_sales': y_test_pred_stacked})
    path = convert_filename('submission/sub-{}-{}-{}'.format('Stacked', estimator_name, start_time)) + '.csv'
    submission_df.to_csv(path, index = False)
    print('{}: Saved submission to {}'.format(current_time(), path))

    coefs = pd.DataFrame(data = dict(zip(x_train.columns, estimator.coef_)), index = [0])
    coefs.to_csv(convert_filename('results/feature_importances-{}-{}-{}'.format('Stacked', estimator_name, start_time)) + '.csv', index = False)
    print('First Level Importance: \n {}'.format(coefs))

train_meta(LinearRegression(), val_preds, y_val_real, test_preds)
train_meta(SGDRegressor(penalty = 'l2',random_state = SEED), val_preds, y_val_real, test_preds)

train_meta(LinearRegression(), val_preds.drop(columns = ['LinearRegression']), y_val_real, test_preds.drop(columns = ['LinearRegression']))
train_meta(SGDRegressor(penalty = 'l2',random_state = SEED), val_preds.drop(columns = ['LinearRegression']), y_val_real, test_preds.drop(columns = ['LinearRegression']))

train_meta(LinearRegression(), val_preds.drop(columns = ['LinearRegression', 'DecisionTreeRegressor']), y_val_real, test_preds.drop(columns = ['LinearRegression', 'DecisionTreeRegressor']))
train_meta(SGDRegressor(penalty = 'l2',random_state = SEED), val_preds.drop(columns = ['LinearRegression', 'DecisionTreeRegressor']), y_val_real, test_preds.drop(columns = ['LinearRegression', 'DecisionTreeRegressor']))
