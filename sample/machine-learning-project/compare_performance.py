""" import libraries """
import numpy as np
import pandas as pd
import os
from util import *
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
%matplotlib inline

import seaborn as sns
# Set figure aesthetics
plt.style.use('fivethirtyeight')
PLOT_HEIGHT = 6
PLOT_WIDTH = PLOT_HEIGHT * 1.618
plt.rcParams["figure.figsize"] = [PLOT_WIDTH,PLOT_HEIGHT]


setting_filenames = [filename for filename in os.listdir(TRAIN_KEEP_PATH) if filename.startswith('setting')]
scores = [filename.split('.')[0].split('-') for filename in setting_filenames]
scores = pd.DataFrame(scores)
scores['Time'] = scores[2].astype(str) + scores[3].astype(str) + scores[4].astype(str)
scores = scores[[5,6, 'Time']]
scores.columns = ['Private Score', 'Public Score', 'Time']
scores['Private Score'] = scores['Private Score'].str.replace('_', '.').astype(float)
scores['Public Score'] = scores['Public Score'].str.replace('_', '.').astype(float)
scores['Score Difference'] = scores['Private Score'] - scores['Public Score']
scores['Weighted Score'] = np.round((scores['Private Score']**2 *0.69 + scores['Public Score']**2 *0.31) ** 0.5, 5)
settings = [pd.read_csv(os.path.join(TRAIN_KEEP_PATH, filename)).set_index('Unnamed: 0') for filename in setting_filenames]
settings = pd.concat(settings, axis = 1).transpose().reset_index().drop(columns = ['index'])

combine = pd.concat([settings, scores], axis = 1)

runtime_filesnames = [filename for filename in os.listdir(TRAIN_KEEP_PATH) if filename.startswith('Run')]
runtimes_time = [filename.split('.')[0].split('-') for filename in runtime_filesnames]
runtimes_time = pd.DataFrame(runtimes_time)
runtimes_time['Time'] = runtimes_time[2].astype(str) + runtimes_time[3].astype(str) + runtimes_time[4].astype(str)
runtimes_time = runtimes_time[['Time']]
runtimes = [load_obj(filename.replace('.pkl', '')) for filename in runtime_filesnames]
runtimes = pd.DataFrame(runtimes)
runtimes['Run Time Total (min)'] = runtimes.sum(axis = 1) / 60
runtimes = pd.concat([runtimes_time, runtimes[['Run Time Total (min)']]], axis = 1)

combine = combine.merge(runtimes)

cv_filenames = [filename for filename in os.listdir(TRAIN_KEEP_PATH) if filename.startswith('nwrmsle')]
times = [filename.split('.')[0].split('-') for filename in cv_filenames]
times = pd.DataFrame(times)
times['Time'] = times[2].astype(str) + times[3].astype(str) + times[4].astype(str)
cvs = [pd.read_csv(os.path.join(TRAIN_KEEP_PATH, filename)).iloc[-1:] for filename in cv_filenames]
cvs = pd.concat(cvs, ignore_index=True)
cvs = pd.concat([cvs, times[['Time']]], axis = 1)

combine = combine.merge(cvs).drop(columns = ['seed', 'Score Difference'])
combine = combine[combine['use_specific_columns'] == 'False']

combine.sort_values(['Weighted Score'])
combine.sort_values(['estimator_name', 'lr_full', 'Weighted Score'])
combine.sort_values(['Weighted Score']).drop_duplicates(subset = ['estimator_name']).to_csv('test.csv')
combine.sort_values(['estimator_name', 'lr_full', 'Weighted Score']).to_csv('test.csv')

best_models = combine.sort_values(['Weighted Score']).drop_duplicates(subset = ['estimator_name'])[['estimator_name', 'Private Score', 'Public Score', 'Val NWRMSLE']]
best_models = pd.melt(best_models, id_vars=['estimator_name'], value_vars=['Private Score', 'Public Score', 'Val NWRMSLE'])


sns.catplot(data = best_models, x = 'estimator_name', y = 'value', hue = 'variable', kind = 'bar', height = PLOT_HEIGHT, aspect = 1.6, legend = False)
plt.title('Score Across Models', loc = 'center', y=1.2, fontsize = 20)
plt.tight_layout()
plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=1)
plt.ylim(0.45, 0.65)
plt.xlabel('Model', size = 14)
plt.ylabel('NWRMSLE', size = 14)
plt.savefig('test', dpi=200, bbox_inches="tight")
plt.show()
plt.close()

runtime_models = combine.sort_values(['Weighted Score']).drop_duplicates(subset = ['estimator_name'])[['estimator_name', 'Run Time Total (min)']]
runtime_models.iloc[-1, -1] = 8.329
sns.catplot(data = runtime_models, x = 'estimator_name', y = 'Run Time Total (min)', kind = 'bar', height = PLOT_HEIGHT, aspect = 1.6, legend = False)
plt.title('Runtime Across Models', loc = 'center', y=1.2, fontsize = 20)
plt.tight_layout()
plt.xlabel('Model', size = 14)
plt.ylabel('Run Time Total (min)', size = 14)
plt.savefig('test', dpi=200, bbox_inches="tight")
plt.show()
plt.close()



# axes = best_models.groupby(['estimator_name'])['Private Score'].sum().plot.bar(rot=0, subplots=True, figsize=(10, 6))
# axes[0].set_ylim(0.5, 0.55)
# axes[0].set_xlabel('Model', size = 14)
# axes[0].set_ylabel('NWRMSLE', size = 14)
# axes[0].set_title('Private Score Across Models', size = 18)
# plt.savefig('test', dpi = 200)
#
# axes = best_models.groupby(['estimator_name'])['Val NWRMSLE'].sum().plot.bar(rot=0, subplots=True, figsize=(10, 6))
# axes[0].set_ylim(0.58, 0.63)
# axes[0].set_xlabel('Model', size = 14)
# axes[0].set_ylabel('NWRMSLE', size = 14)
# axes[0].set_title('Validation Score Across Models', size = 18)
# plt.savefig('test', dpi = 200)
#
# axes = best_models.groupby(['estimator_name'])['Public Score'].sum().plot.bar(rot=0, subplots=True, figsize=(10, 6))
# axes[0].set_ylim(0.5, 0.55)
# axes[0].set_xlabel('Model', size = 14)
# axes[0].set_ylabel('NWRMSLE', size = 14)
# axes[0].set_title('Public Score Across Models', size = 18)
# plt.savefig('test', dpi = 200)





# # best params
# param_filenames = [filename for filename in os.listdir(TRAIN_KEEP_PATH) if filename.startswith('GridSearch-Best_Params-DecisionTreeRegressor')]
# days = [filename.split('.')[0].split('-') for filename in param_filenames]
# days = pd.DataFrame(days)
# days['Day'] = days[3].str.replace('Day', '').astype(int)
#
# params = [load_obj(filename.replace('.pkl', '')) for filename in param_filenames]
# params = pd.DataFrame(params)
#
# combine = pd.concat([days[['Day']], params], axis = 1)
# combine.sort_values(['Day']).transpose().to_csv('test.csv')
