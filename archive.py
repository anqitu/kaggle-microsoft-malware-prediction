# Inspiration: https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823
# Inspiration: https://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution
# For each fold of the train:
#   Train on other folds, validate with current fold to find best iteration
#   Predict for train and test with the best iteration
# Calcualte the average of all folds as the final prediction
def train_predict_lgb(estimator, train_x, train_y, test_x):
    train_predict= np.zeros(len(train_x))
    test_predict = np.zeros(len(test_x))
    importances = np.zeros(train_x.shape[1])
    FOLDS = 5
    folds = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)

    count = 0
    for train_index, val_index in folds.split(train_x, train_y):
        # TRAIN LGBM
        count += 1
        print_info('LightGBM FOLD {}'.format(count))
        train_x_A, train_x_B = train_x.iloc[train_index], train_x.iloc[val_index]
        train_y_A, train_y_B = train_y.iloc[train_index], train_y.iloc[val_index]

        # estimator.fit(train_x_A, train_y_A, eval_metric='auc', eval_set=[(train_x_B, train_y_B)], verbose=200, early_stopping_rounds=100)
        estimator.fit(train_x_A, train_y_A, eval_metric='auc', eval_set=[(train_x_B, train_y_B)], verbose=200, early_stopping_rounds=10)

        # PREDICT TEST
        del train_x_A, train_x_B, train_y_A, train_y_B
        gc.collect()

        print_info('Making Prediction')
        train_predict += estimator.predict_proba(train_x,num_iteration=estimator.best_iteration_)[:, -1]/FOLDS
        test_predict += estimator.predict_proba(test_x,num_iteration=estimator.best_iteration_)[:, -1]/FOLDS
        importances += estimator.feature_importances_/FOLDS
    return estimator, importances, train_predict, test_predict
