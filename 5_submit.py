# Setting
SAMPLE = False

# Import libraries
from setting import *
from util import *

import gc
import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K

os.environ['PYTHONHASHSEED']=str(SEED)
tf.set_random_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Activation
from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, Callback
import lightgbm as lgb

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.options.display.max_rows = 100
pd.options.display.max_columns = 100
pd.options.display.width = 1000


# Set Global Random Seed -------------------------------------------------------
print_title('Set Global Random Seed')
np.random.seed(SEED)
print_info('Set global random seed to {}'.format(SEED))

# Load data --------------------------------------------------------------------
print_title('Load Data')
print_info('Start loading data')

if not os.path.isfile(TRAIN_DATA_PATH):
    print_info("File '{}' does not exist.".format(TRAIN_DATA_PATH))
    print_info("Please download it according to the instructions in README.md")
    quit()

if SAMPLE:
    train_df = pd.read_csv(TRAIN_DATA_PATH, nrows = 30000, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])
    test_df = pd.read_csv(TEST_DATA_PATH, nrows = 1, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])
else:
    train_df = pd.read_csv(TRAIN_DATA_PATH, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])
    test_df = pd.read_csv(TEST_DATA_PATH, dtype=ORIGIN_LOAD_TYPES, na_values=['UNKNOWN', 'Unknown'])

print_info('Finish loading data')
test_ids = test_df['MachineIdentifier']

# Preprocess data
from preprocess import *
train_len = train_df.shape[0]
for df in [train_df, test_df]:
    select_features(df)
    fill_missing_values(df)
    aggregate(df)
    map_values(df)
    handle_invalid_values(df)
    remove_outliers(df)

combine_df = train_df.append(test_df)
combine_df['Census_OSEdition'] = combine_df['Census_OSEdition'].fillna(combine_df['Census_OSEdition'].mode()[0])

encode_categorical_values(combine_df)
train_df = combine_df.iloc[:train_len]
test_df = combine_df.iloc[train_len:]

train_y = train_df[TARGET]
train_x = train_df.drop(columns = TARGET)
test_x = test_df.drop(columns = TARGET)

train_x = train_x.drop(columns = ['Census_OSVersion', 'AvSigVersion'])
test_x = test_x.drop(columns = ['Census_OSVersion', 'AvSigVersion'])
normalize_data(train_x, test_x)

del(combine_df, train_df, test_df)
gc.collect()

# Train model
def submit(estimator, train_x, train_y, test_x):
    try:
        estimator_name = estimator.__name_
        try:
            model = estimator(**load_obj(os.path.join(RESULTS_FT_DIRECTORY, 'Params-{}.pkl'.format(estimator_name))))
            print_info('Loaded finetuned params for {}'.format(estimator_name))
        except:
            print('No params found for model {}'.format(estimator_name))
            model = estimator()
    except:
        # BaggingClassifier
        estimator_name = estimator.__class__.__name__
        model = estimator


    print_info('Start fitting {}'.format(estimator_name))
    model.fit(train_x, train_y)

    print_info('Start predicting with {}'.format(estimator_name))
    y_pred = model.predict_proba(test_x)[:,-1]

    submit_df = pd.DataFrame(data = {'MachineIdentifier': test_ids, 'HasDetections': y_pred})
    submit_df.to_csv(os.path.join(RESULTS_SUBMISSION_DIRECTORY, '{}.csv'.format(estimator_name)), index = False)
    print_info('Saved submission file for {}'.format(estimator_name))

submit(LogisticRegression, train_x, train_y, test_x)
submit(DecisionTreeClassifier, train_x, train_y, test_x)
submit(GaussianNB, train_x, train_y, test_x)
submit(RandomForestClassifier, train_x, train_y, test_x)
submit(lgb.LGBMClassifier, train_x, train_y, test_x)

lgbm = lgb.LGBMClassifier(**load_obj(os.path.join(RESULTS_FT_DIRECTORY, 'Params-{}.pkl'.format('LGBMClassifier'))))
submit(BaggingClassifier(lgbm, random_state=SEED, n_jobs=-1, n_estimators=10), train_x, train_y, test_x)

def build_nn(train_x):
    model = Sequential()

    model.add(Dense(128,input_dim=train_x.shape[1]))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(128))
    model.add(Dropout(0.4))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auroc])

    return model

from tensorflow import numpy_function, double, unique, rint
def auroc(y_true, y_pred):
    return numpy_function(roc_auc_score, (y_true, y_pred), double)

estimator_name = 'NeuralNetwork'
model = build_nn(train_x)
print_info('Start fitting {}'.format(estimator_name))
model.fit(train_x, train_y, epochs = 1, batch_size=128)

print_info('Start predicting with {}'.format(estimator_name))
y_pred = model.predict_proba(test_x)[:,-1]

submit_df = pd.DataFrame(data = {'MachineIdentifier': test_ids, 'HasDetections': y_pred})
submit_df.to_csv(os.path.join(RESULTS_SUBMISSION_DIRECTORY, '{}.csv'.format(estimator_name)), index = False)
print_info('Saved submission file for {}'.format(estimator_name))




# submit = pd.read_csv(TEST_DATA_PATH, usecols=['MachineIdentifier','AvSigVersion'])
# lgbm = pd.read_csv(os.path.join(RESULTS_SUBMISSION_DIRECTORY, '{}.csv'.format('LGBMClassifier')),usecols=['HasDetections'])
#
# submit['HasDetections'] = lgbm['HasDetections']
# submit['ASV2'] = submit['AvSigVersion'].map(lambda x: np.int(x.split('.')[1]) )
# submit['ASV3'] = submit['AvSigVersion'].map(lambda x: np.int(x.split('.')[2]) )
# submit.loc[ (submit['ASV2']==281) & (submit['ASV3'] >= 451),'HasDetections'] = 0.4375
# submit[['MachineIdentifier','HasDetections']].to_csv('BestSolution.csv', index=False)
